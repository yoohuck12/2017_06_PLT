{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf340
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww18200\viewh11300\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 1.  Installing apk, removing cache and getting log file:\
	Have a look at page_speed/bash.sh \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
2.  All necessary scripts to analyze the log file:\
	tests/analysis_t\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 3. Convert the original log directly output from WProf-instrumented  mobile browser\
\
We need to split it to small log files, each represents a page load. Note that it might ask you to install some perl packages such as JSON.\
\
empty/create dep_logs, temp_files, graphs, data and logs\
\
Default: rm -rf temp_files/*;rm -rf dep_logs/*; rm -rf data/*;  rm -rf graphs/*, rm logs/*, rm -rf logs_pro\
\
put the log file into logs directory under wprof/dependency-tools/analysis_t \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
4.  For mobile browser,\
\
$ perl slicemobile-old.pl  DIRECTORY_NAME_THAT_CONTAINS_LOG_FILES\
\
The sliced files will be stored in the same subdirectory as the original directory with suffix _pro.\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
5. Prepare sliced files\
\
Add the folder that contains the sliced log files to data/\
\
Default: mv ./logs_pro  data/wprof_300_5_pro_1\
\
Configure the path of the added folder in ProcessMain.pm, search for "test" and see where configuration should be done.\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
6. Run the analysis.\
\
$ perl analyze.pl\
\
Some log files generate malformed JSON. Analyzed info will not be generated for these files.\
We are using this script only to generate the JSON files in graph directory.\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
7. At this point, we only need the graph directory.\
we empty the ./temp_files/wprof_300_5_pro_1/ directory: rm -rf ./temp_files/wprof_300_5_pro_1/*\
then we run json_dag.py: \'93python json_dag.py\'94  (python version >=3.3)\
this script generate the final temp_files directory containing the analyzed output files per each website.\
\
\
Results are stored in graphs/*. Each file in graphs/* is in the json format that represents the dependency graph of a Web page. There's also the results with specifics on each resource like download time and size in dep_logs/ and other intermediate information in temp_files/.}